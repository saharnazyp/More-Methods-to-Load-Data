{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# More Methods to Load Data in Google Colab and Jupyter Notebook","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading Data from a URL or API\nIf your data is hosted on a website or an API, you can use requests or urllib to fetch the data:\n\n**Load CSV File from a URL:**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nurl = 'https://example.com/data.csv'\ndf = pd.read_csv(url)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Data Using requests:**","metadata":{}},{"cell_type":"code","source":"import requests\nresponse = requests.get('https://example.com/data')\ndata = response.json()  # If the data is in JSON format\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Using gdown to Download Files from Google Drive\nIf your files are on Google Drive but you don’t want to mount the drive, you can use gdown to download them directly.\n\n**Install gdown:**","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Download Files from Google Drive: Convert the shared link to an ID:**","metadata":{}},{"cell_type":"code","source":"!gdown 'https://drive.google.com/uc?id=your_file_id'\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Loading Data from Databases\nYou can connect to various databases using libraries like sqlite3, psycopg2, or SQLAlchemy.\n\n**Connecting to SQLite:**","metadata":{}},{"cell_type":"code","source":"import sqlite3\nconn = sqlite3.connect('database.db')\ndf = pd.read_sql_query(\"SELECT * FROM table_name\", conn)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Connecting to PostgreSQL:","metadata":{}},{"cell_type":"code","source":"import psycopg2\nconn = psycopg2.connect(\n    host=\"localhost\",\n    database=\"your_db\",\n    user=\"your_user\",\n    password=\"your_password\"\n)\ndf = pd.read_sql_query(\"SELECT * FROM table_name\", conn)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Using Google Sheets\nTo load data from Google Sheets, you can use the Google Sheets API or simpler libraries like gspread.\n\n**Install and Use gspread: First, install the library:**","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade gspread\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Then use the following code to load data:**","metadata":{}},{"cell_type":"code","source":"import gspread\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\n# Authenticate\nauth.authenticate_user()\ngc = gspread.authorize(GoogleCredentials.get_application_default())\n\n# Open Google Sheet\nspreadsheet = gc.open('your_google_sheet_name')\nworksheet = spreadsheet.sheet1\n\n# Fetch data\nrows = worksheet.get_all_values()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Using Kaggle to Download Datasets\nIf you are using datasets from Kaggle, you can download them directly in Google Colab.\n\n**Install Kaggle Library:**","metadata":{}},{"cell_type":"code","source":"!pip install kaggle\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Upload Kaggle API Token and Download Dataset:**","metadata":{}},{"cell_type":"code","source":"from google.colab import files\nfiles.upload()  # Upload kaggle.json\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n# Download Dataset\n!kaggle datasets download -d dataset-owner/dataset-name\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Using FTP to Fetch Data\nIf your data is hosted on an FTP server, you can use the ftplib library to connect and fetch the data:\n\n**Connecting to an FTP Server:**","metadata":{}},{"cell_type":"code","source":"from ftplib import FTP\n\nftp = FTP('ftp.example.com')\nftp.login(user='username', passwd='password')\n\n# Download file\nwith open('local_filename', 'wb') as f:\n    ftp.retrbinary('RETR remote_filename', f.write)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Using Dropbox to Download Files\nYou can use the Dropbox API or libraries like dropbox to access files stored on Dropbox.\n\n**Install Dropbox Library:**","metadata":{}},{"cell_type":"code","source":"!pip install dropbox\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download File from Dropbox:","metadata":{}},{"cell_type":"code","source":"import dropbox\n\ndbx = dropbox.Dropbox('your_access_token')\n\nwith open(\"filename\", \"wb\") as f:\n    metadata, res = dbx.files_download('/dropbox_path/filename')\n    f.write(res.content)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional Methods for Uploading and Loading Data in Google Colab and Jupyter Notebook","metadata":{}},{"cell_type":"markdown","source":"# 1. Using Azure Blob Storage or AWS S3\nIf your data is stored in cloud storage services like Azure or AWS, you can use the respective libraries to fetch data:\n\n**Azure Blob Storage:**","metadata":{}},{"cell_type":"code","source":"from azure.storage.blob import BlobServiceClient\n\nconnect_str = \"your_connection_string\"\nblob_service_client = BlobServiceClient.from_connection_string(connect_str)\nblob_client = blob_service_client.get_blob_client(container=\"your_container\", blob=\"your_blob\")\n\nwith open(\"filename\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AWS S3:","metadata":{}},{"cell_type":"code","source":"import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('your_bucket', 'your_key', 'filename')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Using BigQuery\n**If your data is stored in Google BigQuery, you can use the bigquery library:**","metadata":{}},{"cell_type":"code","source":"from google.cloud import bigquery\n\nclient = bigquery.Client()\nquery = \"SELECT * FROM `your_project.your_dataset.your_table`\"\ndf = client.query(query).to_dataframe()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Using HDF5 and Parquet\nIf your data is stored in more advanced formats like HDF5 or Parquet, you can use related libraries:\n\n**Load HDF5 File:**","metadata":{}},{"cell_type":"code","source":"import h5py\n\nwith h5py.File('filename.h5', 'r') as f:\n    data = f['dataset_name'][:]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Parquet File:**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet('filename.parquet')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Using Google Cloud Storage (GCS)\n**If you're using Google Cloud Storage (GCS), you can use the google-cloud-storage library:**","metadata":{}},{"cell_type":"code","source":"from google.cloud import storage\n\nclient = storage.Client()\nbucket = client.get_bucket('your_bucket_name')\nblob = bucket.blob('your_blob_name')\nblob.download_to_filename('filename')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Using Web Scraping\nIf your data comes from websites, you can use web scraping tools like BeautifulSoup or Selenium to fetch the data:\n\n**Using BeautifulSoup:**","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\nurl = 'https://example.com'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\ndata = soup.find_all('your_target_element')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Loading Excel Files\nTo load Excel data, you can use libraries like pandas or openpyxl:\n\n**Load Excel File:**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_excel('filename.xlsx', sheet_name='Sheet1')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Using Google Cloud Datalab\nGoogle Cloud Datalab is an advanced interactive tool designed for data analysis, machine learning, and visualization. It integrates with Google Cloud services such as BigQuery, Cloud Storage, and Machine Learning Engine, making it a powerful platform for handling large datasets and running analytics in the cloud.\n\n**To use Google Cloud Datalab for loading and analyzing data, follow these steps:**","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Set Up Datalab\nYou need to set up and deploy Datalab within your Google Cloud Project:","metadata":{}},{"cell_type":"code","source":"gcloud components install datalab\ngcloud datalab create my-datalab-instance\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Access Datalab\nOnce deployed, access Datalab in your browser, where it will open a Jupyter Notebook-like interface for running Python and SQL code.\n# Step 3: Loading Data from BigQuery\nIn Datalab, you can run BigQuery queries directly:","metadata":{}},{"cell_type":"code","source":"%%bigquery\nSELECT * FROM `your_project.your_dataset.your_table`\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Loading Data from Google Cloud Storage\nTo load data from Google Cloud Storage (GCS):","metadata":{}},{"cell_type":"code","source":"from google.cloud import storage\n\nclient = storage.Client()\nbucket = client.get_bucket('your_bucket_name')\nblob = bucket.blob('your_blob_name')\nblob.download_to_filename('filename')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Machine Learning with TensorFlow\nYou can also run machine learning models using TensorFlow or other libraries integrated with Google Cloud’s Machine Learning Engine:","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Load data, train model, etc.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Using Google Sheets","metadata":{}},{"cell_type":"markdown","source":"Google Sheets is another powerful tool that allows you to store and work with data in a spreadsheet format. You can easily load and manipulate data from Google Sheets in Google Colab or Jupyter Notebook using the gspread library or `pandas com\n\nHere’s how to load data from Google Sheet\n\n**Step 1: Install Install gspread and Authenticate**\nYou need to install gspread and authen","metadata":{}},{"cell_type":"code","source":"!pip install gspread\n!pip install gspread_dataframe\n!pip install --upgrade gspread oauth2client\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, you need to set up authentication using the Google Cloud console by creating a project and enabling the Google Sheets API. Once you've obtained the credentials, you can upload them to Colab.\n\nStep 2: Authenticate and Connect to Google Sheets\nUse the following code to authenticate:","metadata":{}},{"cell_type":"code","source":"import gspread\nfrom oauth2client.service_account import ServiceAccountCredentials\n\n# Define the scope\nscope = [\"https://spreadsheets.google.com/feeds\",'https://www.googleapis.com/auth/spreadsheets',\n         \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n\n# Add credentials to the account\ncreds = ServiceAccountCredentials.from_json_keyfile_name('your_credentials_file.json', scope)\n\n# Authorize the client\nclient = gspread.authorize(creds)\n\n# Access the Google Sheet\nsheet = client.open('Your_Spreadsheet_Name').sheet1\n\n# Get all the records from the sheet\ndata = sheet.get_all_records()\n\n# Display the data\nprint(data)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Using Pandas to Read Google Sheets\nYou can also load Google Sheets data directly into a pandas DataFrame:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport gspread\nfrom gspread_dataframe import get_as_dataframe\nfrom oauth2client.service_account import ServiceAccountCredentials\n\n# Authenticate and access the sheet\ncreds = ServiceAccountCredentials.from_json_keyfile_name('your_credentials_file.json', scope)\nclient = gspread.authorize(creds)\n\n# Access the Google Sheet\nsheet = client.open('Your_Spreadsheet_Name').sheet1\n\n# Load the data into a pandas DataFrame\ndf = get_as_dataframe(sheet)\nprint(df.head())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Reading Data with Google Sheets API via pandas\nAlternatively, you can use the pandas library to fetch data directly from Google Sheets without using gspread:","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade gspread pandas\n\nimport pandas as pd\n\n# Google Sheets URL\nsheet_url = 'https://docs.google.com/spreadsheets/d/YOUR_SPREADSHEET_ID/edit#gid=0'\n\n# Load the sheet into a pandas DataFrame\ncsv_export_url = sheet_url.replace('/edit#gid=', '/gviz/tq?tqx=out:csv&gid=')\ndf = pd.read_csv(csv_export_url)\n\n# View the first few rows\nprint(df.head())\n","metadata":{},"execution_count":null,"outputs":[]}]}